{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#\n",
    "#  GRADIENT DESCENT\n",
    "! pip install hurry.filesize\n",
    "#from IPython.display import Image\n",
    "#from IPython.core.display import HTML \n",
    "#from scipy.stats import binom\n",
    "#from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory taken by: 10000000000 datapoints with 80 features, each requireng 8 bytes of memory is 6T GB\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# STOCHASTIC GRADIENT DESCENT\n",
    "# IF you have very big datasets, it is expensive to compute the gradient of all of them at each iteration\n",
    "# the stochastic gradient descent assumes that\n",
    "# 1) at each iteration a point si drawn from the data set using uniform probability\n",
    "# 2) the gradient is computed only for the single point drawn\n",
    "# 3) the next step is based on moving from w_t based on the gradient computed for the random dataset point\n",
    "# 4) at the enext iteration the process is repeated, drawing again from tehe dataset\n",
    "#\n",
    "# The process\n",
    "# 1) does converge (though in a quite erratic way) if the step size is reasonable\n",
    "# 2) the step size needs to be shrinked and it follows as order 1/(t+1)\n",
    "#\n",
    "#\n",
    "# This library takes care of conversions\n",
    "# The SI system considers increased rounded at 1000 perfectly\n",
    "from hurry.filesize import size, si\n",
    "#\n",
    "bytes_per_entry = 8                    # bytes per entry\n",
    "number_of_features = 80                # columns, i.e. number of features\n",
    "datapoints_number  = 10000000000       # number of datapoints\n",
    "total_memory =bytes_per_entry * number_of_features * datapoints_number\n",
    "print(f\"Total memory taken by: {datapoints_number} datapoints with {number_of_features} features, each requireng {bytes_per_entry} bytes of memory is {size(total_memory, system=si)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hurry.filesize\n",
      "  Downloading hurry.filesize-0.9.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from hurry.filesize) (59.6.0)\n",
      "Building wheels for collected packages: hurry.filesize\n",
      "  Building wheel for hurry.filesize (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hurry.filesize: filename=hurry.filesize-0.9-py3-none-any.whl size=4117 sha256=6c84c9b24b99c60835577903372e47353f29e720f22dd2914c4be67fadcc5633\n",
      "  Stored in directory: /home/vale6811/.cache/pip/wheels/e9/af/17/1c4cd045d88f20d450522470819d85349c3388c151af64590b\n",
      "Successfully built hurry.filesize\n",
      "Installing collected packages: hurry.filesize\n",
      "Successfully installed hurry.filesize-0.9\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
